{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a list of the names of given datasets\n",
    "folders = [\"test\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n"
     ]
    }
   ],
   "source": [
    "file_counter = 0\n",
    "#Creates an empty list that will hold data from each dataset\n",
    "sets_list = []\n",
    "#Iterates through dataset names to read in csvs from each dataset folder\n",
    "for dataset in folders:\n",
    "    #Creates an empty list to hold each image from each dataset\n",
    "    matrix_list = []\n",
    "    #Iterates through all csv files contained in each dataset's folder\n",
    "    for file in glob.glob(str(dataset) + '/' + '*.csv'):\n",
    "        file_counter += 1\n",
    "        #Reads in the csv\n",
    "        data = pd.read_csv(file, header = None)\n",
    "        #Makes the image data into a dataframe\n",
    "        df = pd.DataFrame(data)\n",
    "        #Drops the first row of the dataframe that contains labels\n",
    "        df = df.drop(labels = 0, axis = 0)\n",
    "        #Adds the image dataframe to the list of images\n",
    "        matrix_list.append(df)\n",
    "    #Adds the list of images from each dataset to a list containing all datasets\n",
    "    sets_list.append(matrix_list)\n",
    "print(file_counter)\n",
    "#Sets list is a list of lists of dataframes\n",
    "#Sets list contains a list for each dataset which contains dataframes (each dataframe is one image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sets a percentile value to dictate what is defined as a peak (in this case, a peak is > 85 percentile)\n",
    "percentile = 85\n",
    "all_peaks = []\n",
    "for dataset in sets_list:\n",
    "    num_peaks = []\n",
    "    for image in dataset:\n",
    "        #Casts data from each image as a float to deal with type errors\n",
    "        image_np = image.astype(float)    \n",
    "        #Calculates a percentile value for the image based on percentile specified above\n",
    "        percentile_val = np.nanpercentile(image_np, percentile)\n",
    "        \n",
    "        #Creates an empty dataframe and then fills it with the calculated value of the image's percentile\n",
    "        percentile_df = pd.DataFrame(np.zeros([len(image_np), image_np.shape[1]]))\n",
    "        percentile_df = percentile_df.replace(0,percentile_val)\n",
    "    \n",
    "        #Find the difference between each value in the image and the percentile value for the image\n",
    "        diff = image_np - percentile_df\n",
    "        \n",
    "        #Counts the number of values in each image that are greater than the  percentile value\n",
    "        counter = 0\n",
    "        for rowIndex, row in diff.iterrows(): #iterate over rows\n",
    "            for columnIndex, value in row.items():\n",
    "                if value > 0:\n",
    "                    counter += 1\n",
    "\n",
    "        num_peaks.append(counter)\n",
    "    all_peaks.append(num_peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv_data = all_peaks[0] + all_peaks[1] + all_peaks[2] + all_peaks[3] + all_peaks[4] + all_peaks[5]\n",
    "csv_data = all_peaks[0]\n",
    "df = pd.DataFrame(csv_data, columns = [\"num_peaks\"])\n",
    "df.index = np.arange(1, len(df) + 1)\n",
    "df.to_csv(\"test_num_peaks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Creates a list so that we can collect max, min, mean gradients for each dataset (as well as stdev of gradients)\n",
    "stats = [np.max, np.min, np.mean, np.std]\n",
    "op_gradients = []\n",
    "#Iterates through every operation we want to perform on the gradients\n",
    "for op in stats:\n",
    "    gradient = []\n",
    "    #Iterates through the 4 datasets\n",
    "    for dataset in sets_list:\n",
    "        image_grads = []\n",
    "         #Iterates through each image contained within each of the datasets\n",
    "        for image in dataset:\n",
    "            col = []\n",
    "            #Iterates through each column of pixel values within each image\n",
    "            for column in image:\n",
    "                #Converts to float to avoid type errors\n",
    "                column_np = image[column].astype(float)\n",
    "                #If the last column is reached, the gradient is written as zero to avoid comparing to out of range columns\n",
    "                if column == (image.shape[1] - 1):\n",
    "                    col.append(0)\n",
    "                #Else, if the column is not the last in the image\n",
    "                else:\n",
    "                     #compares the current column to the next column in the image\n",
    "                    next_col = image[column + 1].astype(float)\n",
    "                    #Takes gradients between the current and next columns (over both axes)\n",
    "                    # And performs one of the statistical operations for each column\n",
    "                    col.append(op(np.abs(np.gradient([column_np, next_col]))))\n",
    "            #Makes a list of gradient value for each image\n",
    "            image_grads.append(op(col))\n",
    "        #Adds the list of gradient values from each image in each dataset into a list that contains all datasets gradients\n",
    "        gradient.append(image_grads)\n",
    "    #Adds all the dataset values to a list that contains one entry for each operation that was performed\n",
    "    op_gradients.append(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv_max = op_gradients[0][0] + op_gradients[0][1] + op_gradients[0][2] + op_gradients[0][3] + op_gradients[0][4] + op_gradients[0][5]\n",
    "#csv_min = op_gradients[1][0] + op_gradients[1][1] + op_gradients[1][2] + op_gradients[1][3] + op_gradients[1][4] + op_gradients[1][5]\n",
    "#csv_mean = op_gradients[2][0] + op_gradients[2][1] + op_gradients[2][2] + op_gradients[2][3] + op_gradients[2][4] + op_gradients[2][5]\n",
    "#csv_std = op_gradients[3][0] + op_gradients[3][1] + op_gradients[3][2] + op_gradients[3][3] + op_gradients[3][4] + op_gradients[3][5]\n",
    "csv_max = op_gradients[0][0]\n",
    "csv_min = op_gradients[1][0]\n",
    "csv_mean = op_gradients[2][0]\n",
    "csv_std = op_gradients[3][0]\n",
    "\n",
    "df_max = pd.DataFrame(csv_max, columns = [\"gradient_max\"])\n",
    "df_min = pd.DataFrame(csv_min, columns = [\"gradient_min\"])\n",
    "df_mean = pd.DataFrame(csv_mean, columns = [\"gradient_mean\"])\n",
    "df_std = pd.DataFrame(csv_std, columns = [\"gradient_stdev\"])\n",
    "\n",
    "gradient_df = pd.concat([df_max, df_min, df_mean, df_std], axis = 1)\n",
    "gradient_df.index = np.arange(1, len(gradient_df) + 1)\n",
    "\n",
    "gradient_df.to_csv(\"test_gradient.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
